{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward сети\n",
    "\n",
    "Итак, давайте потренируемся тренировать нейронные сети прямого распространения (так как делали на паре)\n",
    "При этом попробуем создать свою функцию активации на одном из слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем необходимые импорты\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Загрузим датасет CIFAR-100, сразу же создадим dataloader для него\n",
    "# Если вам не хватает вычислительных ресурсов, то можно вернуться к CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True,  \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True,)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=16,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создайте собственную архитектуру! Можете использовать все, что угодно, но в ограничении на использование линейные слои (пока без сверток)\n",
    "# Давайте добавим ограниченный Leaky_relu, то есть output = max(0.1x, 0.5x)\n",
    "# Ваша задача добавить его в архитектуру сети как функцию активации\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def __lrelu(self, x, low=0.1, high=0.5):\n",
    "        return torch.max(x * low, x * high)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.fc3(x)\n",
    "        x = self.__lrelu(x)\n",
    "        x = self.fc4(x)\n",
    "        #x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "net = Net(32 * 32 * 3, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,   301] loss: 0.344\n",
      "[1,   601] loss: 0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:10<01:34, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 0.001\n",
      "[2,   301] loss: 0.309\n",
      "[2,   601] loss: 0.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:20<01:21, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,     1] loss: 0.001\n",
      "[3,   301] loss: 0.289\n",
      "[3,   601] loss: 0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:31<01:15, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,     1] loss: 0.001\n",
      "[4,   301] loss: 0.278\n",
      "[4,   601] loss: 0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:44<01:07, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,     1] loss: 0.001\n",
      "[5,   301] loss: 0.269\n",
      "[5,   601] loss: 0.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:55<00:57, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,     1] loss: 0.001\n",
      "[6,   301] loss: 0.260\n",
      "[6,   601] loss: 0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:07<00:46, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,     1] loss: 0.001\n",
      "[7,   301] loss: 0.253\n",
      "[7,   601] loss: 0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [01:20<00:35, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,     1] loss: 0.001\n",
      "[8,   301] loss: 0.248\n",
      "[8,   601] loss: 0.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [01:32<00:24, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,     1] loss: 0.001\n",
      "[9,   301] loss: 0.241\n",
      "[9,   601] loss: 0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [01:46<00:12, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,     1] loss: 0.001\n",
      "[10,   301] loss: 0.237\n",
      "[10,   601] loss: 0.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:57<00:00, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Запустить обучение (по аналогии с тем, что делали на паре)\n",
    "for epoch in tqdm(range(10)):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(data_iter)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.2728e-01,  2.9940e-02,  9.1230e-01,  2.0052e+00, -2.3041e-01,\n",
       "          1.5666e+00,  1.4700e+00, -2.8651e+00,  5.8745e-01, -1.9406e+00],\n",
       "        [ 1.8165e+00,  3.3776e+00, -1.4165e+00, -1.4577e+00, -1.6867e+00,\n",
       "         -2.2428e+00, -3.2711e+00, -1.7588e+00,  3.7623e+00,  3.8530e+00],\n",
       "        [ 2.3779e+00,  2.8355e+00, -7.9422e-01, -1.4283e+00, -1.2667e+00,\n",
       "         -1.9438e+00, -4.3288e+00, -1.5453e+00,  4.0085e+00,  2.4073e+00],\n",
       "        [ 2.1591e+00,  1.0463e+00,  4.3292e-01, -9.2424e-01, -5.0109e-01,\n",
       "         -1.0192e+00, -3.3220e+00, -7.2781e-01,  2.5441e+00,  4.8883e-01],\n",
       "        [-7.4158e-01, -2.0179e+00,  1.8666e+00,  6.3143e-01,  2.4917e+00,\n",
       "          7.3997e-01,  2.2789e+00, -2.1409e-02, -8.8384e-01, -2.2130e+00],\n",
       "        [-1.3101e+00, -1.6838e-01,  6.9766e-01,  1.8987e+00,  6.6608e-01,\n",
       "          1.4950e+00,  2.6361e+00, -2.6033e-01, -2.8196e+00, -3.8427e-01],\n",
       "        [-1.3896e+00,  1.1168e+00, -1.8360e-01,  2.5398e+00, -1.3319e+00,\n",
       "          2.3896e+00,  7.6129e-01, -1.4551e+00, -8.8921e-01, -5.2509e-01],\n",
       "        [-7.1971e-01, -1.8998e+00,  2.2001e+00,  7.1152e-01,  2.0994e+00,\n",
       "          7.4108e-01,  2.4256e+00,  3.9694e-01, -2.1127e+00, -2.0603e+00],\n",
       "        [ 4.8933e-01, -6.1459e-01,  1.5644e+00,  7.9458e-01,  1.1914e+00,\n",
       "          1.1985e+00, -3.7923e-01, -1.6324e-02, -7.6884e-01, -1.9783e+00],\n",
       "        [ 1.1059e+00,  3.8527e+00, -1.2749e+00, -3.6267e-01, -2.2761e+00,\n",
       "         -1.4854e+00, -2.0393e+00, -2.2014e+00,  2.1346e+00,  2.8302e+00],\n",
       "        [ 1.8566e+00, -4.6365e-01,  7.9499e-01, -1.2602e-03, -9.2981e-02,\n",
       "         -3.3850e-01, -1.6012e+00, -2.6249e+00,  3.8407e+00, -8.9085e-01],\n",
       "        [ 3.1308e-01,  3.4881e+00, -2.0505e+00, -1.2022e+00, -1.4827e+00,\n",
       "         -1.8700e+00, -2.0398e+00, -1.6530e-01,  2.0775e+00,  4.3781e+00],\n",
       "        [-1.0270e+00,  1.3523e+00, -2.6519e-01,  7.7656e-01,  9.2664e-02,\n",
       "          8.4375e-01,  3.9685e-01, -2.9585e-01, -2.4598e-01, -1.3747e-01],\n",
       "        [-1.5087e-01,  6.3455e-01, -2.7223e-01, -2.9855e-01,  4.6148e-01,\n",
       "          2.8731e-01, -3.4560e-01,  1.6126e+00, -1.0857e+00,  3.6955e-01],\n",
       "        [ 6.1052e-01,  1.7302e+00, -5.8756e-01, -8.6897e-01, -4.7674e-01,\n",
       "         -1.1908e+00, -1.9532e+00,  8.1544e-01,  5.6103e-01,  2.0236e+00],\n",
       "        [ 1.3403e+00, -9.2048e-01,  9.0265e-01, -4.3503e-01,  4.0545e-01,\n",
       "         -3.3242e-01, -1.4283e+00, -1.1325e+00,  3.0776e+00, -6.1779e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "outputs = net(images)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 8, 8, 4, 6, 3, 6, 2, 1, 8, 9, 1, 7, 9, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, array([3, 8, 8, 0, 6, 6, 1, 6, 3, 1]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(test_loader)\n",
    "true_classes = list()\n",
    "\n",
    "for it in data_iter:\n",
    "    true_classes.extend(it[1])\n",
    "\n",
    "true_classes = np.array(true_classes)\n",
    "len(true_classes), true_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, array([3, 9, 8, 8, 4, 6, 3, 6, 2, 1]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "data_iter = iter(test_loader)\n",
    "pred_classes = list()\n",
    "\n",
    "for it in data_iter:\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    pred_classes.extend(predicted)\n",
    "\n",
    "# pred_classes = np.array([classes[predicted[i]] for i in range(len(labels))])\n",
    "pred_classes = np.array(pred_classes)\n",
    "len(pred_classes), pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.0975\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy = {(true_classes == pred_classes).sum() / len(true_classes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика на тестовой выборке довольна низка. Попробуем улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение слоёв и их количества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8 * hidden_dim)\n",
    "        self.do1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(8 * hidden_dim, 2 * hidden_dim)\n",
    "        self.do2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.do3 = nn.Dropout(0.33)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.do4 = nn.Dropout(0.25)\n",
    "        self.fc5 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "    \n",
    "    def __lrelu(self, x, low=0.1, high=0.5):\n",
    "        return torch.max(x * low, x * high)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.do1(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.do2(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.fc3(x)\n",
    "        x = self.__lrelu(x)\n",
    "        x = self.do3(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.fc4(x)\n",
    "        x = self.__lrelu(x)\n",
    "        x = self.do4(x)\n",
    "        x = F.leaky_relu(x, 0.05)\n",
    "        x = self.fc5(x)\n",
    "        #x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "net = Net(32 * 32 * 3, 128, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сразу поменяем оптимизатор (чтобы два раза не учить)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.ASGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,   301] loss: 0.346\n",
      "[1,   601] loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:18<02:45, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 0.001\n",
      "[2,   301] loss: 0.346\n",
      "[2,   601] loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:36<02:27, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,     1] loss: 0.001\n",
      "[3,   301] loss: 0.346\n",
      "[3,   601] loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:55<02:10, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,     1] loss: 0.001\n",
      "[4,   301] loss: 0.345\n",
      "[4,   601] loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [01:14<01:52, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,     1] loss: 0.001\n",
      "[5,   301] loss: 0.345\n",
      "[5,   601] loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:33<01:34, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,     1] loss: 0.001\n",
      "[6,   301] loss: 0.345\n",
      "[6,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:52<01:14, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,     1] loss: 0.001\n",
      "[7,   301] loss: 0.345\n",
      "[7,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [02:10<00:56, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,     1] loss: 0.001\n",
      "[8,   301] loss: 0.345\n",
      "[8,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [02:29<00:37, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,     1] loss: 0.001\n",
      "[9,   301] loss: 0.345\n",
      "[9,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [02:48<00:18, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,     1] loss: 0.001\n",
      "[10,   301] loss: 0.345\n",
      "[10,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:07<00:00, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, array([3, 8, 8, 0, 6, 6, 1, 6, 3, 1]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(test_loader)\n",
    "true_classes = list()\n",
    "\n",
    "for it in data_iter:\n",
    "    true_classes.extend(it[1])\n",
    "\n",
    "true_classes = np.array(true_classes)\n",
    "len(true_classes), true_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "data_iter = iter(test_loader)\n",
    "pred_classes = list()\n",
    "\n",
    "for it in data_iter:\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    pred_classes.extend(predicted)\n",
    "\n",
    "# pred_classes = np.array([classes[predicted[i]] for i in range(len(labels))])\n",
    "pred_classes = np.array(pred_classes)\n",
    "len(pred_classes), pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.1\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy = {(true_classes == pred_classes).sum() / len(true_classes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть небольшое улучшение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим немного модицикаций исходного изображения. Возможно улучшатся результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomResizedCrop:\n",
    "    def __call__(self, x):\n",
    "        im_w, im_h = x.size\n",
    "        scale = random.uniform(0.5, 1.0)\n",
    "        target_w, target_h = int(im_w * scale), int(im_h * scale)\n",
    "        x1 = random.randint(0, im_w - target_w)\n",
    "        y1 = random.randint(0, im_h - target_h)\n",
    "        x2, y2 = x1 + target_w, y1 + target_h\n",
    "        return x.crop((x1, y1, x2, y2)).resize((im_w, im_h))\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __call__(self, x):\n",
    "        if random.random() < 0.5:\n",
    "            return x.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return x\n",
    "\n",
    "class RandomRotation:\n",
    "    def __init__(self, degrees=15):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return x.rotate(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    RandomResizedCrop(),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True,  \n",
    "                                             transform=train_transform,\n",
    "                                             download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                            train=False,\n",
    "                                            transform=test_transform,\n",
    "                                            download=True,)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=16,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(32 * 32 * 3, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.ASGD(net.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,   301] loss: 0.346\n",
      "[1,   601] loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:22<03:24, 22.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 0.001\n",
      "[2,   301] loss: 0.346\n",
      "[2,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:46<03:06, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,     1] loss: 0.001\n",
      "[3,   301] loss: 0.345\n",
      "[3,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [01:09<02:43, 23.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,     1] loss: 0.001\n",
      "[4,   301] loss: 0.345\n",
      "[4,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [01:33<02:20, 23.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,     1] loss: 0.001\n",
      "[5,   301] loss: 0.345\n",
      "[5,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:56<01:57, 23.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,     1] loss: 0.001\n",
      "[6,   301] loss: 0.345\n",
      "[6,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [02:20<01:34, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,     1] loss: 0.001\n",
      "[7,   301] loss: 0.345\n",
      "[7,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [02:44<01:10, 23.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,     1] loss: 0.001\n",
      "[8,   301] loss: 0.345\n",
      "[8,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [03:08<00:47, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,     1] loss: 0.001\n",
      "[9,   301] loss: 0.345\n",
      "[9,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [03:32<00:23, 23.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,     1] loss: 0.001\n",
      "[10,   301] loss: 0.345\n",
      "[10,   601] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:56<00:00, 23.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, array([3, 8, 8, 0, 6, 6, 1, 6, 3, 1]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(test_loader)\n",
    "true_classes = list()\n",
    "\n",
    "for it in data_iter:\n",
    "    true_classes.extend(it[1])\n",
    "\n",
    "true_classes = np.array(true_classes)\n",
    "len(true_classes), true_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, array([0, 0, 0, 0, 0, 9, 5, 9, 0, 0]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "data_iter = iter(test_loader)\n",
    "pred_classes = list()\n",
    "\n",
    "for it in data_iter:\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    pred_classes.extend(predicted)\n",
    "\n",
    "# pred_classes = np.array([classes[predicted[i]] for i in range(len(labels))])\n",
    "pred_classes = np.array(pred_classes)\n",
    "len(pred_classes), pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.0976\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy = {(true_classes == pred_classes).sum() / len(true_classes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация для полносвязанных слоев не заметно, чтобы эффективно работала. Вернулись почти к первоначальным результатам."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
